import json
import csv
import hashlib
import sys
import os
from collections import Counter

# Add parent directory to path to import normalization module
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
from normalization import normalize_for_search
from solve_ambiguity import resolve_ambiguous, get_resolution_type

# --- Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª ---
INPUT_JSON = "Bukhari/Bukhari_Without_Tashkel_results_advanced_with_matn.json"
OUTPUT_JSON = "Bukhari/Bukhari_Normalized_Ready_For_Graph.json"
UNMAPPED_REPORT = "Bukhari/unmapped_narrators_report.csv"
MAPPINGS_FILE = "narrator_mappings.json"
CONTEXT_MAPPINGS_FILE = "Bukhari/resolved_context_mappings.json"

# Load narrator mappings from JSON file
def load_narrator_mappings():
    """Load narrator name mappings from JSON file."""
    try:
        with open(MAPPINGS_FILE, 'r', encoding='utf-8') as f:
            data = json.load(f)
            # Filter out comment keys (starting with _comment)
            mappings = {k: v for k, v in data['mappings'].items() if not k.startswith('_comment')}
            return mappings, data.get('special_cases', {})
    except FileNotFoundError:
        print(f"âš ï¸  Warning: {MAPPINGS_FILE} not found. Using empty mappings.")
        return {}, {}
    except json.JSONDecodeError as e:
        print(f"âŒ Error parsing {MAPPINGS_FILE}: {e}")
        return {}, {}

def load_context_mappings():
    """Load context-aware disambiguation mappings generated by solve_ambiguity.py."""
    try:
        with open(CONTEXT_MAPPINGS_FILE, 'r', encoding='utf-8') as f:
            data = json.load(f)
            return data.get('context_mappings', {})
    except FileNotFoundError:
        print(f"â„¹ï¸  {CONTEXT_MAPPINGS_FILE} not found. Context resolution disabled.")
        return {}
    except json.JSONDecodeError as e:
        print(f"âŒ Error parsing {CONTEXT_MAPPINGS_FILE}: {e}")
        return {}

# Load mappings at module level
NAME_MAPPING, SPECIAL_CASES = load_narrator_mappings()
CONTEXT_MAPPINGS = load_context_mappings()

# Legacy inline dictionary (kept for reference, but JSON takes precedence)
# Ù‚Ø§Ù…ÙˆØ³ Ø§Ù„ØªÙˆØ­ÙŠØ¯ - Ø§Ù„Ø¢Ù† Ù…Ø­ÙÙˆØ¸ ÙÙŠ narrator_mappings.json
LEGACY_NAME_MAPPING = {
    # === Top 20 Most Frequent (Original + New) ===
    'Ø³ÙÙŠØ§Ù†': 'Ø³ÙÙŠØ§Ù† Ø§Ù„Ø«ÙˆØ±ÙŠ',  # 798 mentions - Ø§Ù„Ø£ÙƒØ«Ø± Ø´ÙŠÙˆØ¹Ø§Ù‹ ÙÙŠ Ø§Ù„Ø¨Ø®Ø§Ø±ÙŠ
    'Ù‡Ø´Ø§Ù…': 'Ù‡Ø´Ø§Ù… Ø¨Ù† Ø¹Ø±ÙˆØ©',  # 428 mentions
    'ÙŠØ­ÙŠÙ‰': 'ÙŠØ­ÙŠÙ‰ Ø¨Ù† Ø³Ø¹ÙŠØ¯ Ø§Ù„Ù‚Ø·Ø§Ù†',  # 407 mentions
    'Ø¹Ø¨Ø¯ Ø§Ù„Ù„Ù‡ Ø¨Ù† Ø¹Ù…Ø±': 'Ø¹Ø¨Ø¯ Ø§Ù„Ù„Ù‡ Ø¨Ù† Ø¹Ù…Ø±',  # 285 mentions - Ø§Ù„Ø´ÙƒÙ„ Ø§Ù„ÙƒØ§Ù…Ù„
    'Ø¥Ø¨Ø±Ø§Ù‡ÙŠÙ…': 'Ø¥Ø¨Ø±Ø§Ù‡ÙŠÙ… Ø§Ù„Ù†Ø®Ø¹ÙŠ',  # 237 mentions
    'Ø¹Ù…Ø±Ùˆ': 'Ø¹Ù…Ø±Ùˆ Ø¨Ù† Ø¯ÙŠÙ†Ø§Ø±',  # 213 mentions
    'Ø¹Ø¨ÙŠØ¯ Ø§Ù„Ù„Ù‡': 'Ø¹Ø¨ÙŠØ¯ Ø§Ù„Ù„Ù‡ Ø¨Ù† Ø¹Ù…Ø±',  # 210 mentions
    'ÙŠØ­ÙŠÙ‰ Ø¨Ù† Ø¨ÙƒÙŠØ±': 'ÙŠØ­ÙŠÙ‰ Ø¨Ù† Ø¨ÙƒÙŠØ±',  # 204 mentions
    'Ø£Ø¨Ùˆ Ø¥Ø³Ø­Ø§Ù‚': 'Ø£Ø¨Ùˆ Ø¥Ø³Ø­Ø§Ù‚ Ø§Ù„Ø³Ø¨ÙŠØ¹ÙŠ',  # 195 mentions
    'Ø¹Ø¨Ø¯ Ø§Ù„Ù„Ù‡ Ø¨Ù† Ù…Ø­Ù…Ø¯': 'Ø¹Ø¨Ø¯ Ø§Ù„Ù„Ù‡ Ø¨Ù† Ù…Ø­Ù…Ø¯ Ø§Ù„Ù…Ø³Ù†Ø¯ÙŠ',  # 188 mentions
    'Ø¹ÙƒØ±Ù…Ø©': 'Ø¹ÙƒØ±Ù…Ø© Ù…ÙˆÙ„Ù‰ Ø§Ø¨Ù† Ø¹Ø¨Ø§Ø³',  # 172 mentions
    'Ø³Ø¹ÙŠØ¯ Ø¨Ù† Ø¬Ø¨ÙŠØ±': 'Ø³Ø¹ÙŠØ¯ Ø¨Ù† Ø¬Ø¨ÙŠØ±',  # 171 mentions
    'ÙŠØ­ÙŠÙ‰ Ø¨Ù† Ø³Ø¹ÙŠØ¯': 'ÙŠØ­ÙŠÙ‰ Ø¨Ù† Ø³Ø¹ÙŠØ¯ Ø§Ù„Ù‚Ø·Ø§Ù†',  # 171 mentions
    'Ø£Ø¨Ùˆ Ø³Ø¹ÙŠØ¯ Ø§Ù„Ø®Ø¯Ø±ÙŠ': 'Ø£Ø¨Ùˆ Ø³Ø¹ÙŠØ¯ Ø§Ù„Ø®Ø¯Ø±ÙŠ',  # 168 mentions
    'Ø¢Ø¯Ù…': 'Ø¢Ø¯Ù… Ø¨Ù† Ø£Ø¨ÙŠ Ø¥ÙŠØ§Ø³',  # 163 mentions
    'Ù…Ø­Ù…Ø¯': 'Ù…Ø­Ù…Ø¯ Ø¨Ù† Ø³ÙŠØ±ÙŠÙ†',  # 163 mentions - Ø§ÙØªØ±Ø§Ø¶
    'Ø§Ù„Ø£Ø¹Ø±Ø¬': 'Ø¹Ø¨Ø¯ Ø§Ù„Ø±Ø­Ù…Ù† Ø¨Ù† Ù‡Ø±Ù…Ø² Ø§Ù„Ø£Ø¹Ø±Ø¬',  # 160 mentions
    'Ø£Ø¨Ùˆ Ø§Ù„Ø²Ù†Ø§Ø¯': 'Ø¹Ø¨Ø¯ Ø§Ù„Ù„Ù‡ Ø¨Ù† Ø°ÙƒÙˆØ§Ù† Ø£Ø¨Ùˆ Ø§Ù„Ø²Ù†Ø§Ø¯',  # 159 mentions

    # === Original High-Frequency Names ===
    'Ø£Ø¨Ùˆ Ù‡Ø±ÙŠØ±Ø©': 'Ø£Ø¨Ùˆ Ù‡Ø±ÙŠØ±Ø©',
    'Ø¹Ø§Ø¦Ø´Ø©': 'Ø¹Ø§Ø¦Ø´Ø© Ø¨Ù†Øª Ø£Ø¨ÙŠ Ø¨ÙƒØ±',
    'Ø´Ø¹Ø¨Ø©': 'Ø´Ø¹Ø¨Ø© Ø¨Ù† Ø§Ù„Ø­Ø¬Ø§Ø¬',
    'Ø§Ù„Ø²Ù‡Ø±ÙŠ': 'Ù…Ø­Ù…Ø¯ Ø¨Ù† Ù…Ø³Ù„Ù… Ø§Ù„Ø²Ù‡Ø±ÙŠ',
    'Ø§Ø¨Ù† Ø´Ù‡Ø§Ø¨': 'Ù…Ø­Ù…Ø¯ Ø¨Ù† Ù…Ø³Ù„Ù… Ø§Ù„Ø²Ù‡Ø±ÙŠ',
    'Ø§Ø¨Ù† Ø¹Ø¨Ø§Ø³': 'Ø¹Ø¨Ø¯ Ø§Ù„Ù„Ù‡ Ø¨Ù† Ø¹Ø¨Ø§Ø³',
    'Ù…Ø§Ù„Ùƒ': 'Ù…Ø§Ù„Ùƒ Ø¨Ù† Ø£Ù†Ø³',
    'Ø§Ø¨Ù† Ø¹Ù…Ø±': 'Ø¹Ø¨Ø¯ Ø§Ù„Ù„Ù‡ Ø¨Ù† Ø¹Ù…Ø±',
    'Ø£Ù†Ø³ Ø¨Ù† Ù…Ø§Ù„Ùƒ': 'Ø£Ù†Ø³ Ø¨Ù† Ù…Ø§Ù„Ùƒ',
    'Ø£Ù†Ø³': 'Ø£Ù†Ø³ Ø¨Ù† Ù…Ø§Ù„Ùƒ',
    'Ù†Ø§ÙØ¹': 'Ù†Ø§ÙØ¹ Ù…ÙˆÙ„Ù‰ Ø§Ø¨Ù† Ø¹Ù…Ø±',
    'Ø§Ù„Ù„ÙŠØ«': 'Ø§Ù„Ù„ÙŠØ« Ø¨Ù† Ø³Ø¹Ø¯',
    'Ø§Ù„Ø£Ø¹Ù…Ø´': 'Ø³Ù„ÙŠÙ…Ø§Ù† Ø¨Ù† Ù…Ù‡Ø±Ø§Ù† Ø§Ù„Ø£Ø¹Ù…Ø´',
    'Ù‚ØªØ§Ø¯Ø©': 'Ù‚ØªØ§Ø¯Ø© Ø¨Ù† Ø¯Ø¹Ø§Ù…Ø©',
    'Ø¹Ø¨Ø¯ Ø§Ù„Ù„Ù‡ Ø¨Ù† ÙŠÙˆØ³Ù': 'Ø¹Ø¨Ø¯ Ø§Ù„Ù„Ù‡ Ø¨Ù† ÙŠÙˆØ³Ù Ø§Ù„ØªÙ†ÙŠØ³ÙŠ',
    'Ø¹Ø±ÙˆØ© Ø¨Ù† Ø§Ù„Ø²Ø¨ÙŠØ±': 'Ø¹Ø±ÙˆØ© Ø¨Ù† Ø§Ù„Ø²Ø¨ÙŠØ±',
    'Ø¹Ø±ÙˆØ©': 'Ø¹Ø±ÙˆØ© Ø¨Ù† Ø§Ù„Ø²Ø¨ÙŠØ±',
    'Ù…Ø¹Ù…Ø±': 'Ù…Ø¹Ù…Ø± Ø¨Ù† Ø±Ø§Ø´Ø¯',
    'Ù…Ø³Ø¯Ø¯': 'Ù…Ø³Ø¯Ø¯ Ø¨Ù† Ù…Ø³Ø±Ù‡Ø¯',
    'Ø¹Ù„ÙŠ Ø¨Ù† Ø¹Ø¨Ø¯ Ø§Ù„Ù„Ù‡': 'Ø¹Ù„ÙŠ Ø¨Ù† Ø¹Ø¨Ø¯ Ø§Ù„Ù„Ù‡ Ø§Ù„Ù…Ø¯ÙŠÙ†ÙŠ',
    'Ø¬Ø§Ø¨Ø± Ø¨Ù† Ø¹Ø¨Ø¯ Ø§Ù„Ù„Ù‡': 'Ø¬Ø§Ø¨Ø± Ø¨Ù† Ø¹Ø¨Ø¯ Ø§Ù„Ù„Ù‡',
    'Ø§Ø¨Ù† Ø¬Ø±ÙŠØ¬': 'Ø¹Ø¨Ø¯ Ø§Ù„Ù…Ù„Ùƒ Ø¨Ù† Ø¬Ø±ÙŠØ¬',
    'Ù…ÙˆØ³Ù‰ Ø¨Ù† Ø¥Ø³Ù…Ø§Ø¹ÙŠÙ„': 'Ù…ÙˆØ³Ù‰ Ø¨Ù† Ø¥Ø³Ù…Ø§Ø¹ÙŠÙ„ Ø§Ù„ØªØ¨ÙˆØ°ÙƒÙŠ',
    'Ù‡Ø´Ø§Ù… Ø¨Ù† Ø¹Ø±ÙˆØ©': 'Ù‡Ø´Ø§Ù… Ø¨Ù† Ø¹Ø±ÙˆØ©',
    'Ø³Ø¹ÙŠØ¯ Ø¨Ù† Ø§Ù„Ù…Ø³ÙŠØ¨': 'Ø³Ø¹ÙŠØ¯ Ø¨Ù† Ø§Ù„Ù…Ø³ÙŠØ¨',
    'Ø£Ø¨Ùˆ Ø³Ù„Ù…Ø©': 'Ø£Ø¨Ùˆ Ø³Ù„Ù…Ø© Ø¨Ù† Ø¹Ø¨Ø¯ Ø§Ù„Ø±Ø­Ù…Ù†',
    'Ù‚ØªÙŠØ¨Ø© Ø¨Ù† Ø³Ø¹ÙŠØ¯': 'Ù‚ØªÙŠØ¨Ø© Ø¨Ù† Ø³Ø¹ÙŠØ¯',
    'Ø­Ù…Ø§Ø¯ Ø¨Ù† Ø²ÙŠØ¯': 'Ø­Ù…Ø§Ø¯ Ø¨Ù† Ø²ÙŠØ¯',
    'Ø¥Ø³Ù…Ø§Ø¹ÙŠÙ„': 'Ø¥Ø³Ù…Ø§Ø¹ÙŠÙ„ Ø¨Ù† Ø£Ø¨ÙŠ Ø£ÙˆÙŠØ³',  # âš ï¸ Ø§ÙØªØ±Ø§Ø¶ - Ù‚Ø¯ ÙŠØ­ØªØ§Ø¬ ØªØ¯Ù‚ÙŠÙ‚
    'Ø£Ø¨Ùˆ Ù†Ø¹ÙŠÙ…': 'Ø£Ø¨Ùˆ Ù†Ø¹ÙŠÙ… Ø§Ù„ÙØ¶Ù„ Ø¨Ù† Ø¯ÙƒÙŠÙ†',
    'Ø£Ø¨Ùˆ Ø£Ø³Ø§Ù…Ø©': 'Ø£Ø¨Ùˆ Ø£Ø³Ø§Ù…Ø© Ø­Ù…Ø§Ø¯ Ø¨Ù† Ø£Ø³Ø§Ù…Ø©',
    'Ù…Ù†ØµÙˆØ±': 'Ù…Ù†ØµÙˆØ± Ø¨Ù† Ø§Ù„Ù…Ø¹ØªÙ…Ø±',
    'Ø¹Ù‚ÙŠÙ„': 'Ø¹Ù‚ÙŠÙ„ Ø¨Ù† Ø®Ø§Ù„Ø¯',
    'ÙŠÙˆÙ†Ø³': 'ÙŠÙˆÙ†Ø³ Ø¨Ù† ÙŠØ²ÙŠØ¯',
    'Ø´Ø¹ÙŠØ¨': 'Ø´Ø¹ÙŠØ¨ Ø¨Ù† Ø£Ø¨ÙŠ Ø­Ù…Ø²Ø©',
    'Ø£ÙŠÙˆØ¨': 'Ø£ÙŠÙˆØ¨ Ø§Ù„Ø³Ø®ØªÙŠØ§Ù†ÙŠ',
    'Ø§Ù„Ø­ÙƒÙ… Ø¨Ù† Ù†Ø§ÙØ¹': 'Ø§Ù„Ø­ÙƒÙ… Ø¨Ù† Ù†Ø§ÙØ¹',
    'Ù…Ø­Ù…Ø¯ Ø¨Ù† Ø¨Ø´Ø§Ø±': 'Ù…Ø­Ù…Ø¯ Ø¨Ù† Ø¨Ø´Ø§Ø± Ø¨Ù†Ø¯Ø§Ø±',

    # === Additional Common Narrators (Based on Bukhari Frequency) ===
    'Ø­Ù…Ø§Ø¯': 'Ø­Ù…Ø§Ø¯ Ø¨Ù† Ø²ÙŠØ¯',  # ÙŠØ±Ø¬Ø­ Ø£Ù†Ù‡ Ø­Ù…Ø§Ø¯ Ø¨Ù† Ø²ÙŠØ¯ ÙÙŠ Ø§Ù„Ø¨Ø®Ø§Ø±ÙŠ
    'Ø³Ø¹ÙŠØ¯': 'Ø³Ø¹ÙŠØ¯ Ø¨Ù† Ø§Ù„Ù…Ø³ÙŠØ¨',  # Ø§Ù„Ø£ÙƒØ«Ø± Ø´ÙŠÙˆØ¹Ø§Ù‹
    'ÙˆÙ‡Ø¨': 'ÙˆÙ‡Ø¨ Ø¨Ù† Ø¬Ø±ÙŠØ±',
    'Ø§Ø¨Ù† Ø§Ù„Ù…Ø¨Ø§Ø±Ùƒ': 'Ø¹Ø¨Ø¯ Ø§Ù„Ù„Ù‡ Ø¨Ù† Ø§Ù„Ù…Ø¨Ø§Ø±Ùƒ',
    'Ø§Ø¨Ù† Ø£Ø¨ÙŠ Ø°Ø¦Ø¨': 'Ù…Ø­Ù…Ø¯ Ø¨Ù† Ø¹Ø¨Ø¯ Ø§Ù„Ø±Ø­Ù…Ù† Ø¨Ù† Ø£Ø¨ÙŠ Ø°Ø¦Ø¨',
    'Ø§Ù„Ø«ÙˆØ±ÙŠ': 'Ø³ÙÙŠØ§Ù† Ø§Ù„Ø«ÙˆØ±ÙŠ',
    'Ø³ÙÙŠØ§Ù† Ø¨Ù† Ø¹ÙŠÙŠÙ†Ø©': 'Ø³ÙÙŠØ§Ù† Ø¨Ù† Ø¹ÙŠÙŠÙ†Ø©',
    'Ø§Ø¨Ù† ÙˆÙ‡Ø¨': 'Ø¹Ø¨Ø¯ Ø§Ù„Ù„Ù‡ Ø¨Ù† ÙˆÙ‡Ø¨',
    'Ø£Ø¨Ùˆ Ù…Ø¹Ø§ÙˆÙŠØ©': 'Ø£Ø¨Ùˆ Ù…Ø¹Ø§ÙˆÙŠØ© Ù…Ø­Ù…Ø¯ Ø¨Ù† Ø®Ø§Ø²Ù…',
    'ÙˆÙƒÙŠØ¹': 'ÙˆÙƒÙŠØ¹ Ø¨Ù† Ø§Ù„Ø¬Ø±Ø§Ø­',
    'Ø¹Ø¨Ø¯ Ø§Ù„Ø±Ø²Ø§Ù‚': 'Ø¹Ø¨Ø¯ Ø§Ù„Ø±Ø²Ø§Ù‚ Ø¨Ù† Ù‡Ù…Ø§Ù…',
    'ÙŠØ²ÙŠØ¯ Ø¨Ù† Ù‡Ø§Ø±ÙˆÙ†': 'ÙŠØ²ÙŠØ¯ Ø¨Ù† Ù‡Ø§Ø±ÙˆÙ†',
    'Ø­Ø¬Ø§Ø¬ Ø¨Ù† Ù…Ù†Ù‡Ø§Ù„': 'Ø­Ø¬Ø§Ø¬ Ø¨Ù† Ù…Ù†Ù‡Ø§Ù„',
    'Ø£Ø¨Ùˆ Ø¨ÙƒØ± Ø¨Ù† Ø£Ø¨ÙŠ Ø´ÙŠØ¨Ø©': 'Ø£Ø¨Ùˆ Ø¨ÙƒØ± Ø¨Ù† Ø£Ø¨ÙŠ Ø´ÙŠØ¨Ø©',
    'Ù…Ø­Ù…Ø¯ Ø¨Ù† Ø¹Ø¨Ø¯ Ø§Ù„Ù„Ù‡': 'Ù…Ø­Ù…Ø¯ Ø¨Ù† Ø¹Ø¨Ø¯ Ø§Ù„Ù„Ù‡ Ø§Ù„Ø£Ù†ØµØ§Ø±ÙŠ',
    'Ø¹Ø¨Ø¯ Ø§Ù„Ù„Ù‡ Ø¨Ù† Ù…Ø³Ù„Ù…Ø©': 'Ø¹Ø¨Ø¯ Ø§Ù„Ù„Ù‡ Ø¨Ù† Ù…Ø³Ù„Ù…Ø© Ø§Ù„Ù‚Ø¹Ù†Ø¨ÙŠ',
    'Ø£Ø¨Ùˆ Ø§Ù„ÙŠÙ…Ø§Ù†': 'Ø£Ø¨Ùˆ Ø§Ù„ÙŠÙ…Ø§Ù† Ø§Ù„Ø­ÙƒÙ… Ø¨Ù† Ù†Ø§ÙØ¹',
    'ØµØ¯Ù‚Ø© Ø¨Ù† Ø§Ù„ÙØ¶Ù„': 'ØµØ¯Ù‚Ø© Ø¨Ù† Ø§Ù„ÙØ¶Ù„',
    'Ù…Ø­Ù…Ø¯ Ø¨Ù† ÙŠÙˆØ³Ù': 'Ù…Ø­Ù…Ø¯ Ø¨Ù† ÙŠÙˆØ³Ù Ø§Ù„ÙØ±ÙŠØ§Ø¨ÙŠ',
    'Ø£Ø¨Ùˆ Ø§Ù„ÙˆÙ„ÙŠØ¯': 'Ø£Ø¨Ùˆ Ø§Ù„ÙˆÙ„ÙŠØ¯ Ù‡Ø´Ø§Ù… Ø¨Ù† Ø¹Ø¨Ø¯ Ø§Ù„Ù…Ù„Ùƒ',

    # === Round 2: Next Most Frequent (from latest analysis) ===
    'Ø¬Ø±ÙŠØ±': 'Ø¬Ø±ÙŠØ± Ø¨Ù† Ø¹Ø¨Ø¯ Ø§Ù„Ø­Ù…ÙŠØ¯',  # 159 mentions
    'Ù‚ØªÙŠØ¨Ø©': 'Ù‚ØªÙŠØ¨Ø© Ø¨Ù† Ø³Ø¹ÙŠØ¯',  # 145 mentions - short form
    'Ù‡Ù…Ø§Ù…': 'Ù‡Ù…Ø§Ù… Ø¨Ù† ÙŠØ­ÙŠÙ‰',  # 145 mentions
    'Ø³Ù„ÙŠÙ…Ø§Ù† Ø¨Ù† Ø­Ø±Ø¨': 'Ø³Ù„ÙŠÙ…Ø§Ù† Ø¨Ù† Ø­Ø±Ø¨',  # 143 mentions
    'Ø£Ø¨Ùˆ ÙˆØ§Ø¦Ù„': 'Ø£Ø¨Ùˆ ÙˆØ§Ø¦Ù„ Ø´Ù‚ÙŠÙ‚ Ø¨Ù† Ø³Ù„Ù…Ø©',  # 141 mentions
    'Ø³Ù„ÙŠÙ…Ø§Ù†': 'Ø³Ù„ÙŠÙ…Ø§Ù† Ø¨Ù† Ø­Ø±Ø¨',  # 140 mentions - ÙŠØ±Ø¬Ø­
    'Ø¹Ø·Ø§Ø¡': 'Ø¹Ø·Ø§Ø¡ Ø¨Ù† Ø£Ø¨ÙŠ Ø±Ø¨Ø§Ø­',  # 139 mentions
    'Ø£Ø¨Ùˆ Ø­Ø§Ø²Ù…': 'Ø£Ø¨Ùˆ Ø­Ø§Ø²Ù… Ø³Ù„Ù…Ø© Ø¨Ù† Ø¯ÙŠÙ†Ø§Ø±',  # 134 mentions
    'Ø³Ø§Ù„Ù…': 'Ø³Ø§Ù„Ù… Ø¨Ù† Ø¹Ø¨Ø¯ Ø§Ù„Ù„Ù‡ Ø¨Ù† Ø¹Ù…Ø±',  # 131 mentions
    'Ø¥Ø¨Ø±Ø§Ù‡ÙŠÙ… Ø¨Ù† Ø³Ø¹Ø¯': 'Ø¥Ø¨Ø±Ø§Ù‡ÙŠÙ… Ø¨Ù† Ø³Ø¹Ø¯',  # 130 mentions
    'Ø£Ø¨Ùˆ Ø³Ù„Ù…Ø© Ø¨Ù† Ø¹Ø¨Ø¯ Ø§Ù„Ø±Ø­Ù…Ù†': 'Ø£Ø¨Ùˆ Ø³Ù„Ù…Ø© Ø¨Ù† Ø¹Ø¨Ø¯ Ø§Ù„Ø±Ø­Ù…Ù†',  # 127 mentions - full form
    'ØºÙ†Ø¯Ø±': 'ØºÙ†Ø¯Ø± Ù…Ø­Ù…Ø¯ Ø¨Ù† Ø¬Ø¹ÙØ±',  # 127 mentions
    'Ø¹Ù„ÙŠ': 'Ø¹Ù„ÙŠ Ø¨Ù† Ø§Ù„Ù…Ø¯ÙŠÙ†ÙŠ',  # 123 mentions - ÙŠØ±Ø¬Ø­ ÙÙŠ Ø´ÙŠÙˆØ® Ø§Ù„Ø¨Ø®Ø§Ø±ÙŠ
    'Ø£Ø¨Ùˆ Ø¹ÙˆØ§Ù†Ø©': 'Ø£Ø¨Ùˆ Ø¹ÙˆØ§Ù†Ø© Ø§Ù„ÙˆØ¶Ø§Ø­ Ø¨Ù† Ø¹Ø¨Ø¯ Ø§Ù„Ù„Ù‡',  # 122 mentions
    'Ù…Ø³Ø±ÙˆÙ‚': 'Ù…Ø³Ø±ÙˆÙ‚ Ø¨Ù† Ø§Ù„Ø£Ø¬Ø¯Ø¹',  # 121 mentions
    'Ø­Ù…ÙŠØ¯': 'Ø­Ù…ÙŠØ¯ Ø§Ù„Ø·ÙˆÙŠÙ„',  # 119 mentions
    'Ù…Ø¬Ø§Ù‡Ø¯': 'Ù…Ø¬Ø§Ù‡Ø¯ Ø¨Ù† Ø¬Ø¨Ø±',  # 118 mentions
    'Ø£Ø¨Ø§Ù†': 'Ø£Ø¨Ø§Ù† Ø¨Ù† ÙŠØ²ÙŠØ¯',  # common in Bukhari
    'Ø¬Ø±ÙŠØ± Ø¨Ù† Ø­Ø§Ø²Ù…': 'Ø¬Ø±ÙŠØ± Ø¨Ù† Ø­Ø§Ø²Ù…',
    'Ø¹Ù…Ø± Ø¨Ù† Ø­ÙØµ': 'Ø¹Ù…Ø± Ø¨Ù† Ø­ÙØµ Ø¨Ù† ØºÙŠØ§Ø«',

    # === Round 3: Push to 60%+ Coverage ===
    'Ø¥Ø³Ø­Ø§Ù‚': 'Ø¥Ø³Ø­Ø§Ù‚ Ø¨Ù† Ø±Ø§Ù‡ÙˆÙŠÙ‡',  # 117 mentions - Ø£Ùˆ Ø¥Ø³Ø­Ø§Ù‚ Ø¨Ù† Ù…Ù†ØµÙˆØ±
    'Ø¹Ø¨Ø¯Ø§Ù†': 'Ø¹Ø¨Ø¯Ø§Ù† Ø¨Ù† Ø¹Ø«Ù…Ø§Ù†',  # 114 mentions
    'ÙˆÙ‡ÙŠØ¨': 'ÙˆÙ‡ÙŠØ¨ Ø¨Ù† Ø®Ø§Ù„Ø¯',  # 112 mentions
    'Ø¬Ø§Ø¨Ø±': 'Ø¬Ø§Ø¨Ø± Ø¨Ù† Ø¹Ø¨Ø¯ Ø§Ù„Ù„Ù‡',  # 112 mentions - same as full form
    'Ø£Ø¨Ùˆ Ù…ÙˆØ³Ù‰': 'Ø£Ø¨Ùˆ Ù…ÙˆØ³Ù‰ Ø§Ù„Ø£Ø´Ø¹Ø±ÙŠ',  # 110 mentions
    'Ù…Ø­Ù…Ø¯ Ø¨Ù† Ø§Ù„Ù…Ø«Ù†Ù‰': 'Ù…Ø­Ù…Ø¯ Ø¨Ù† Ø§Ù„Ù…Ø«Ù†Ù‰ Ø§Ù„Ø¹Ù†Ø²ÙŠ',  # 107 mentions
    'Ù…ÙˆØ³Ù‰ Ø¨Ù† Ø¹Ù‚Ø¨Ø©': 'Ù…ÙˆØ³Ù‰ Ø¨Ù† Ø¹Ù‚Ø¨Ø©',  # 107 mentions
    'Ø£Ø¨Ùˆ ØµØ§Ù„Ø­': 'Ø£Ø¨Ùˆ ØµØ§Ù„Ø­ Ø§Ù„Ø³Ù…Ø§Ù†',  # 106 mentions - Ø°ÙƒÙˆØ§Ù†
    'Ø§Ù„Ø´Ø¹Ø¨ÙŠ': 'Ø¹Ø§Ù…Ø± Ø¨Ù† Ø´Ø±Ø§Ø­ÙŠÙ„ Ø§Ù„Ø´Ø¹Ø¨ÙŠ',  # 105 mentions
    'Ø®Ø§Ù„Ø¯': 'Ø®Ø§Ù„Ø¯ Ø¨Ù† Ø§Ù„Ø­Ø§Ø±Ø«',  # 103 mentions - Ø§Ù„Ø£ÙƒØ«Ø± ÙÙŠ Ø§Ù„Ø¨Ø®Ø§Ø±ÙŠ
    'Ø£Ø¨Ùˆ Ø§Ù„Ù†Ø¹Ù…Ø§Ù†': 'Ø£Ø¨Ùˆ Ø§Ù„Ù†Ø¹Ù…Ø§Ù† Ù…Ø­Ù…Ø¯ Ø¨Ù† Ø§Ù„ÙØ¶Ù„',  # 98 mentions
    'Ø§Ù„Ø¨Ø±Ø§Ø¡': 'Ø§Ù„Ø¨Ø±Ø§Ø¡ Ø¨Ù† Ø¹Ø§Ø²Ø¨',  # 96 mentions
    'Ø¹Ø¨Ø¯ Ø§Ù„ÙˆØ§Ø±Ø«': 'Ø¹Ø¨Ø¯ Ø§Ù„ÙˆØ§Ø±Ø« Ø¨Ù† Ø³Ø¹ÙŠØ¯',  # 94 mentions
    'Ø¹Ø¨ÙŠØ¯ Ø§Ù„Ù„Ù‡ Ø¨Ù† Ø¹Ø¨Ø¯ Ø§Ù„Ù„Ù‡': 'Ø¹Ø¨ÙŠØ¯ Ø§Ù„Ù„Ù‡ Ø¨Ù† Ø¹Ø¨Ø¯ Ø§Ù„Ù„Ù‡ Ø¨Ù† Ø¹ØªØ¨Ø©',  # 93 mentions
    'Ø¹Ø¨Ø¯ Ø§Ù„Ù„Ù‡ Ø¨Ù† Ø¯ÙŠÙ†Ø§Ø±': 'Ø¹Ø¨Ø¯ Ø§Ù„Ù„Ù‡ Ø¨Ù† Ø¯ÙŠÙ†Ø§Ø±',  # 89 mentions
    'Ø¹Ø¨Ø¯ Ø§Ù„Ø¹Ø²ÙŠØ² Ø¨Ù† Ø¹Ø¨Ø¯ Ø§Ù„Ù„Ù‡': 'Ø¹Ø¨Ø¯ Ø§Ù„Ø¹Ø²ÙŠØ² Ø¨Ù† Ø¹Ø¨Ø¯ Ø§Ù„Ù„Ù‡ Ø§Ù„Ø£ÙˆÙŠØ³ÙŠ',  # 89 mentions
    'Ø¥Ø³Ø±Ø§Ø¦ÙŠÙ„': 'Ø¥Ø³Ø±Ø§Ø¦ÙŠÙ„ Ø¨Ù† ÙŠÙˆÙ†Ø³',  # 85 mentions
    'Ø®Ø§Ù„Ø¯ Ø¨Ù† Ø§Ù„Ø­Ø§Ø±Ø«': 'Ø®Ø§Ù„Ø¯ Ø¨Ù† Ø§Ù„Ø­Ø§Ø±Ø«',  # same as Ø®Ø§Ù„Ø¯
    'Ù…Ø­Ù…ÙˆØ¯': 'Ù…Ø­Ù…ÙˆØ¯ Ø¨Ù† ØºÙŠÙ„Ø§Ù†',
    'Ø¹Ø¨Ø¯ Ø§Ù„Ù„Ù‡ Ø¨Ù† Ø±Ø¬Ø§Ø¡': 'Ø¹Ø¨Ø¯ Ø§Ù„Ù„Ù‡ Ø¨Ù† Ø±Ø¬Ø§Ø¡',
    'Ù…Ø­Ù…Ø¯ Ø¨Ù† Ø³Ù„Ø§Ù…': 'Ù…Ø­Ù…Ø¯ Ø¨Ù† Ø³Ù„Ø§Ù… Ø§Ù„Ø¨ÙŠÙƒÙ†Ø¯ÙŠ',
    'Ø¨Ø´Ø± Ø¨Ù† Ù…Ø­Ù…Ø¯': 'Ø¨Ø´Ø± Ø¨Ù† Ù…Ø­Ù…Ø¯ Ø§Ù„Ø³Ø®ØªÙŠØ§Ù†ÙŠ',
    'Ø£Ø¨Ùˆ ÙƒØ±ÙŠØ¨': 'Ø£Ø¨Ùˆ ÙƒØ±ÙŠØ¨ Ù…Ø­Ù…Ø¯ Ø¨Ù† Ø§Ù„Ø¹Ù„Ø§Ø¡',
    'Ø§Ø¨Ù† Ø£Ø¨ÙŠ Ø¹Ø¯ÙŠ': 'Ù…Ø­Ù…Ø¯ Ø¨Ù† Ø¥Ø¨Ø±Ø§Ù‡ÙŠÙ… Ø¨Ù† Ø£Ø¨ÙŠ Ø¹Ø¯ÙŠ',
    'Ù…Ø­Ù…Ø¯ Ø¨Ù† ÙƒØ«ÙŠØ±': 'Ù…Ø­Ù…Ø¯ Ø¨Ù† ÙƒØ«ÙŠØ± Ø§Ù„Ø¹Ø¨Ø¯ÙŠ',
    'Ø£Ø¨Ùˆ Ù‚Ù„Ø§Ø¨Ø©': 'Ø£Ø¨Ùˆ Ù‚Ù„Ø§Ø¨Ø© Ø¹Ø¨Ø¯ Ø§Ù„Ù„Ù‡ Ø¨Ù† Ø²ÙŠØ¯',
    'Ø·Ù„Ø­Ø©': 'Ø·Ù„Ø­Ø© Ø¨Ù† Ù…ØµØ±Ù',
    'Ø§Ø¨Ù† Ù†Ù…ÙŠØ±': 'Ø¹Ø¨Ø¯ Ø§Ù„Ù„Ù‡ Ø¨Ù† Ù†Ù…ÙŠØ±',
    'Ø²Ù‡ÙŠØ±': 'Ø²Ù‡ÙŠØ± Ø¨Ù† Ù…Ø¹Ø§ÙˆÙŠØ©',
    'Ø§Ù„Ø­Ø³Ù†': 'Ø§Ù„Ø­Ø³Ù† Ø§Ù„Ø¨ØµØ±ÙŠ',
}

def generate_narrator_id(name):
    """
    Generates a consistent ID based on the narrator's name hash.
    Uses SHA-256 with 12 characters to reduce collision probability.
    """
    hash_val = hashlib.sha256(name.encode('utf-8')).hexdigest()[:12].upper()
    return f"NAR_{hash_val}"

def generate_unmapped_report(all_raw_names):
    """
    Generate a report of high-frequency names that weren't mapped.
    This helps identify which names should be added to NAME_MAPPING.
    """
    # Count frequency of each name
    name_counts = Counter(all_raw_names)

    # Find unmapped names
    unmapped = []
    for name, count in name_counts.items():
        normalized = normalize_for_search(name)
        is_mapped = False
        for map_key in NAME_MAPPING.keys():
            if normalize_for_search(map_key) == normalized:
                is_mapped = True
                break
        if not is_mapped:
            unmapped.append((name, count))

    # Sort by frequency
    unmapped.sort(key=lambda x: x[1], reverse=True)

    print(f"\nğŸ“‹ Unmapped names report:")
    print(f"   - Total unmapped unique names: {len(unmapped)}")
    print(f"   - Top 20 most frequent unmapped names:")

    for name, count in unmapped[:20]:
        print(f"      '{name}': '',  # {count} mentions")

    # Save full report to CSV
    print(f"\nğŸ’¾ Saving full unmapped report to {UNMAPPED_REPORT}...")
    with open(UNMAPPED_REPORT, 'w', encoding='utf-8', newline='') as f:
        writer = csv.writer(f)
        writer.writerow(['Name', 'Frequency', 'Suggested Canonical Name (Empty)'])
        for name, count in unmapped:
            writer.writerow([name, count, ''])

    print(f"   âœ… Saved {len(unmapped)} unmapped names to CSV")

def normalize_hadith_data():
    print(f"ğŸ“š Loaded {len(NAME_MAPPING)} static mappings from {MAPPINGS_FILE}")
    if CONTEXT_MAPPINGS:
        print(f"ğŸ§  Loaded {len(CONTEXT_MAPPINGS)} context-aware mappings from {CONTEXT_MAPPINGS_FILE}")
    if SPECIAL_CASES:
        print(f"   - Context-dependent names: {len(SPECIAL_CASES.get('context_dependent', []))}")
        print(f"   - Generic names to handle: {len(SPECIAL_CASES.get('too_generic', []))}")

    print(f"\nğŸ“‚ Loading {INPUT_JSON}...")
    with open(INPUT_JSON, 'r', encoding='utf-8') as f:
        data = json.load(f)

    unique_narrators = {}  # To store ID -> Name mapping
    all_raw_names = []  # Track all narrator mentions for statistics
    context_resolved_count = 0  # True disambiguation (ambiguous name + student context)
    unambiguous_resolved_count = 0  # Simple 1:1 lookups (only one person with this name)
    pronoun_resolved_count = 0  # Pronoun resolution (Ø£Ø¨ÙŠÙ‡ â†’ father's name)
    static_resolved_count = 0
    collision_warnings = []

    print("ğŸ”„ Normalizing narrator names...")
    for hadith in data:
        if "chains" in hadith and isinstance(hadith["chains"], list):
            for chain in hadith["chains"]:
                narrators = chain["narrators"]
                # Capture raw names BEFORE any modification to ensure
                # context keys use original names (not already-normalized ones)
                raw_names = [n["name"].strip() for n in narrators]

                for i, narrator in enumerate(narrators):
                    raw_name = raw_names[i]
                    all_raw_names.append(raw_name)

                    canonical_name = None

                    # Determine student name for context lookups
                    if i > 0:
                        student_raw = raw_names[i - 1]
                    else:
                        student_raw = "Ø§Ù„Ø¨Ø®Ø§Ø±ÙŠ (Ø¨Ø¯Ø§ÙŠØ© Ø§Ù„Ø³Ù†Ø¯)"

                    # --- STEP 1: Context JSON lookup (pre-computed mappings) ---
                    if CONTEXT_MAPPINGS:
                        context_key = f"{raw_name}|{student_raw}"
                        if context_key in CONTEXT_MAPPINGS:
                            resolved = CONTEXT_MAPPINGS[context_key]
                            if '(ØºØ§Ù…Ø¶)' not in resolved and '(Ø³ÙŠØ§Ù‚ÙŠ)' not in resolved:
                                canonical_name = resolved
                                # Classify by resolution type for honest statistics
                                res_type = get_resolution_type(raw_name)
                                if res_type == 'unambiguous':
                                    unambiguous_resolved_count += 1
                                elif res_type == 'pronoun':
                                    pronoun_resolved_count += 1
                                else:
                                    context_resolved_count += 1

                    # --- STEP 1b: Live rule resolution (covers pairs not in CSV) ---
                    if canonical_name is None:
                        resolved = resolve_ambiguous(raw_name, student_raw)
                        if resolved and '(ØºØ§Ù…Ø¶)' not in resolved and '(Ø³ÙŠØ§Ù‚ÙŠ)' not in resolved:
                            canonical_name = resolved
                            # Classify by resolution type for honest statistics
                            res_type = get_resolution_type(raw_name)
                            if res_type == 'unambiguous':
                                unambiguous_resolved_count += 1
                            elif res_type == 'pronoun':
                                pronoun_resolved_count += 1
                            else:
                                context_resolved_count += 1

                    # --- STEP 2: Static mapping fallback ---
                    if canonical_name is None:
                        normalized_name = normalize_for_search(raw_name)
                        for map_key, map_value in NAME_MAPPING.items():
                            if normalize_for_search(map_key) == normalized_name:
                                canonical_name = map_value
                                static_resolved_count += 1
                                break

                    # --- STEP 3: Identity fallback (keep original) ---
                    if canonical_name is None:
                        canonical_name = raw_name

                    # ØªØ­Ø¯ÙŠØ« Ø§Ù„Ø§Ø³Ù… ÙÙŠ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
                    narrator["original_name"] = raw_name
                    narrator["name"] = canonical_name

                    # ID generation (SHA-256, 12 chars)
                    narrator_id = generate_narrator_id(canonical_name)
                    narrator["id"] = narrator_id

                    # Collision detection
                    if narrator_id in unique_narrators:
                        if unique_narrators[narrator_id] != canonical_name:
                            warning = f"âš ï¸  ID COLLISION: {narrator_id}\n   Existing: {unique_narrators[narrator_id]}\n   New: {canonical_name}"
                            if warning not in collision_warnings:
                                collision_warnings.append(warning)
                    else:
                        unique_narrators[narrator_id] = canonical_name

    mapped_count = unambiguous_resolved_count + context_resolved_count + pronoun_resolved_count + static_resolved_count

    print(f"\nğŸ“Š Statistics:")
    print(f"   - Total narrator mentions: {len(all_raw_names)}")
    print(f"   - Unique narrators (after normalization): {len(unique_narrators)}")
    print(f"   - Unambiguous (1:1 lookup, e.g. Ø¹Ø§Ø¦Ø´Ø©): {unambiguous_resolved_count}")
    print(f"   - Context-disambiguated (student-based, e.g. Ø³ÙÙŠØ§Ù†): {context_resolved_count}")
    print(f"   - Pronoun-resolved (e.g. Ø£Ø¨ÙŠÙ‡ â†’ father): {pronoun_resolved_count}")
    print(f"   - Static-resolved (general mapping): {static_resolved_count}")
    print(f"   - Total mapped: {mapped_count}")

    if len(all_raw_names) > 0:
        coverage = (mapped_count / len(all_raw_names)) * 100
        print(f"   - Coverage: {coverage:.1f}% of mentions mapped")

    if collision_warnings:
        print(f"\nğŸš¨ WARNING: {len(collision_warnings)} ID collisions detected:")
        for warning in collision_warnings[:5]:  # Show first 5
            print(warning)

    # Generate report of unmapped high-frequency names
    generate_unmapped_report(all_raw_names)

    print(f"\nğŸ“Š Unique narrators processed: {len(unique_narrators)}")
    
    print(f"\nğŸ’¾ Saving to {OUTPUT_JSON}...")
    with open(OUTPUT_JSON, 'w', encoding='utf-8') as f:
        json.dump(data, f, ensure_ascii=False, indent=2)
    print(f"   âœ… Saved normalized data")

    # Ø­ÙØ¸ Ù‚Ø§Ø¦Ù…Ø© Ø§Ù„Ø±ÙˆØ§Ø© Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠØ© (Nodes List) Ù„Ø³Ù‡ÙˆÙ„Ø© Ø§Ù„Ø§Ø³ØªÙŠØ±Ø§Ø¯ ÙÙŠ Neo4j
    nodes_file = "Bukhari/narrators_nodes.csv"
    print(f"\nğŸ’¾ Saving nodes list to {nodes_file}...")
    with open(nodes_file, 'w', encoding='utf-8', newline='') as f:
        writer = csv.writer(f)
        writer.writerow(['narrator_id', 'canonical_name'])
        for nid, name in sorted(unique_narrators.items()):
            writer.writerow([nid, name])
    print(f"   âœ… Saved {len(unique_narrators)} unique narrator nodes")

    print("\n" + "="*60)
    print("âœ… Normalization Complete!")
    print("="*60)
    print(f"ğŸ“ Output files:")
    print(f"   - {OUTPUT_JSON}")
    print(f"   - {nodes_file}")
    print(f"   - {UNMAPPED_REPORT}")
    print("\nğŸ’¡ Next steps:")
    print("   1. Review unmapped_narrators_report.csv")
    print("   2. Add high-frequency names to NAME_MAPPING dictionary")
    print("   3. Re-run this script to improve coverage")
    print("   4. When satisfied, proceed to Neo4j ingestion")

if __name__ == "__main__":
    normalize_hadith_data()